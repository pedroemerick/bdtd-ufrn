{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18290,
     "status": "ok",
     "timestamp": 1560989769891,
     "user": {
      "displayName": "Franklin Matheus",
      "photoUrl": "https://lh3.googleusercontent.com/-Rxb0HD2LMVE/AAAAAAAAAAI/AAAAAAAAADg/5rpsKrNME7M/s64/photo.jpg",
      "userId": "10408548631089952245"
     },
     "user_tz": 180
    },
    "id": "YenTPljk3EH-",
    "outputId": "ae8b1f8f-0927-4af4-bcea-b41c8875e66c"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install networkx\n",
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1776,
     "status": "ok",
     "timestamp": 1560975144515,
     "user": {
      "displayName": "Franklin Matheus",
      "photoUrl": "https://lh3.googleusercontent.com/-Rxb0HD2LMVE/AAAAAAAAAAI/AAAAAAAAADg/5rpsKrNME7M/s64/photo.jpg",
      "userId": "10408548631089952245"
     },
     "user_tz": 180
    },
    "id": "CcJuFC50AGSD",
    "outputId": "afe454d7-b54d-44bb-a74a-b0f4e481993b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "#nltk.download('universal_tagset')\n",
    "#nltk.download('treebank')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very well'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6O_KjwzUGvhD"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    pos = {\"J\": wn.ADJ, \"N\": wn.NOUN, \"V\": wn.VERB, \"R\": wn.ADV}\n",
    "    \n",
    "    # Very long abstracts are ignored\n",
    "    try:\n",
    "        if Translator().detect(text).lang == 'pt':\n",
    "            text = Translator().translate(text,src='pt').text\n",
    "        elif Translator().detect(text).lang == 'es':\n",
    "            text = Translator().translate(text,src='es').text\n",
    "        elif Translator().detect(text).lang == 'fr':\n",
    "            text = Translator().translate(text,src='fr').text\n",
    "    except Exception as e:\n",
    "        text = ''\n",
    "    \n",
    "    # We run through the tokenized words joint with their respective pos tag\n",
    "    for token in pos_tag(word_tokenize(text)):\n",
    "    \n",
    "        # For each token, we need to check if it isn't a stopword or a punctuation character\n",
    "        if token[0].lower() not in stopwords.words('english') and token[0] not in string.punctuation:\n",
    "\n",
    "            tag = pos.get(token[1][0],None)\n",
    "\n",
    "            # We'll work only with nouns and adjectives\n",
    "            if tag == wn.NOUN or tag == wn.ADJ:\n",
    "\n",
    "                # We lemmatize the token based on it pos tag\n",
    "                lemma = WordNetLemmatizer().lemmatize(token[0], tag)\n",
    "                if len(lemma) > 1:\n",
    "                  # Finally, we add the lemmatized token into the list to be returned\n",
    "                  result.append(lemma.lower())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1030,
     "status": "ok",
     "timestamp": 1560989566871,
     "user": {
      "displayName": "Franklin Matheus",
      "photoUrl": "https://lh3.googleusercontent.com/-Rxb0HD2LMVE/AAAAAAAAAAI/AAAAAAAAADg/5rpsKrNME7M/s64/photo.jpg",
      "userId": "10408548631089952245"
     },
     "user_tz": 180
    },
    "id": "XOV0dO-lXAU3",
    "outputId": "e3f976aa-e0a0-4a03-9d36-9c34f5235c69"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "def main_topic_terms(tokens,nwords=5):\n",
    "    if len(tokens) > 0:\n",
    "        # First we make a list with the list of tokens\n",
    "        texts = [tokens]\n",
    "\n",
    "        # We use the class Dictionary to map normalized words to their ids \n",
    "        texts_dictionary = Dictionary(texts)\n",
    "\n",
    "        # Convert the document into the bag-of-words format\n",
    "        corpus = [texts_dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "        # And now, we build the LDA model\n",
    "        lda_model = LdaModel(corpus, num_topics=1, id2word=texts_dictionary, passes=10, alpha='auto')\n",
    "\n",
    "        # Finally, with the topics on our hands, we take the terms of the main topic and return them\n",
    "        return [texts_dictionary[id2word[0]] for id2word in lda_model.get_topic_terms(lda_model[corpus[0]][0][0],topn=nwords)]\n",
    "    \n",
    "    # Return a empty list if doesn't exist tokens\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8528,
     "status": "ok",
     "timestamp": 1560979446237,
     "user": {
      "displayName": "Franklin Matheus",
      "photoUrl": "https://lh3.googleusercontent.com/-Rxb0HD2LMVE/AAAAAAAAAAI/AAAAAAAAADg/5rpsKrNME7M/s64/photo.jpg",
      "userId": "10408548631089952245"
     },
     "user_tz": 180
    },
    "id": "rLQGjYYXd23v",
    "outputId": "ca323d5b-2cb5-4183-a7b1-71e6f8c79b0b"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "\n",
    "def get_topics_graph(df):\n",
    "    # We create a new graph (undirected)\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # The magic! We connect every topic words with each other and add the edges to the graph\n",
    "    [graph.add_edges_from(combination) for combination in \n",
    "        [list(combinations(curr,2)) for curr in \n",
    "            [main_topic_terms(preprocess(df.loc[index,'resume'])) \n",
    "                 if row['abstract'] == True else main_topic_terms(preprocess(df.loc[index,'abstract'])) \n",
    "                 for index, row in df.isna().iterrows()\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # Finally, we return the graph\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_graphml(input_dataset,output_graphml):\n",
    "    df = pd.read_csv(input_dataset)\n",
    "    \n",
    "    graph = get_topics_graph(df)\n",
    "    nx.write_graphml(graph, output_graphml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_degree_nodes(graph,n=5):\n",
    "    top_degree = sorted(graph.degree, key=lambda x: x[1], reverse=True)[:n]\n",
    "    [print(curr[1],curr[0]) for curr in top_degree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graphml('datasets/ppgeec.csv','graph/ppgeec.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graphml('datasets/ppgcsa.csv','graph/ppgcsa.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graphml('datasets/ppged.csv','graph/ppged.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 system\n",
      "181 network\n",
      "158 control\n",
      "142 model\n",
      "121 algorithm\n",
      "116 data\n",
      "114 antenna\n",
      "112 method\n",
      "101 structure\n",
      "91 controller\n",
      "1019 nodes, 5436 edges\n"
     ]
    }
   ],
   "source": [
    "graph = nx.read_graphml('graph/ppgeec.graphml')\n",
    "print_degree_nodes(graph,10)\n",
    "print(\"{} nodes, {} edges\".format(len(graph), nx.number_of_edges(graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 group\n",
      "157 patient\n",
      "133 health\n",
      "128 study\n",
      "85 elderly\n",
      "83 woman\n",
      "75 activity\n",
      "63 physical\n",
      "60 cell\n",
      "57 quality\n",
      "1047 nodes, 4215 edges\n"
     ]
    }
   ],
   "source": [
    "graph = nx.read_graphml('graph/ppgcsa.graphml')\n",
    "print_degree_nodes(graph,10)\n",
    "print(\"{} nodes, {} edges\".format(len(graph), nx.number_of_edges(graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 education\n",
      "338 school\n",
      "293 teacher\n",
      "189 student\n",
      "174 research\n",
      "169 knowledge\n",
      "161 study\n",
      "155 practice\n",
      "139 social\n",
      "136 process\n",
      "118 child\n",
      "111 educational\n",
      "996 nodes, 5655 edges\n"
     ]
    }
   ],
   "source": [
    "graph = nx.read_graphml('graph/ppged.graphml')\n",
    "print_degree_nodes(graph,12)\n",
    "print(\"{} nodes, {} edges\".format(len(graph), nx.number_of_edges(graph)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
